{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNT8XI5iwwuyUZn6YHq40Jn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "YnAeq76d6BlN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PvNhmG2SGOe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49a9c0c3-eb81-4f01-d471-e79e23ea0c3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Soy',\n",
              " 'muy',\n",
              " 'proclive',\n",
              " 'a',\n",
              " 'enroscarme',\n",
              " 'en',\n",
              " 'soluciones',\n",
              " 'que',\n",
              " 'no',\n",
              " 'funcionan']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\n",
        "sentences = [\"Soy muy proclive a enroscarme en soluciones que no funcionan\", \"Tengo que cambiar eso\"]\n",
        "\n",
        "\n",
        "sentences[0].split()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "tokenizer.index_word\n",
        "tokenizer.texts_to_sequences(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuWu19jF6Mdp",
        "outputId": "53df0962-8a14-430b-c373-46158bfdf21d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 3, 4, 5, 6, 7, 8, 1, 9, 10], [11, 1, 12, 13]]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = Tokenizer(num_words=6, oov_token='')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "tokenizer.texts_to_sequences(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7aSsWUB6RQh",
        "outputId": "256a8b3e-3e37-493a-ed4a-cda5aa16472b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 4, 5, 1, 1, 1, 1, 2, 1, 1], [1, 2, 1, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding"
      ],
      "metadata": {
        "id": "kK8FQg816bRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: un par de ejemplos cortos y faciles para ilustrar el padding de keras mostrando truncatin, padding, maxlen\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Example with padding\n",
        "padded_sentences = pad_sequences(tokenizer.texts_to_sequences(sentences))\n",
        "print(\"Padding only:\")\n",
        "print(padded_sentences)\n",
        "\n",
        "# Example with padding and maxlen\n",
        "padded_sentences_maxlen = pad_sequences(tokenizer.texts_to_sequences(sentences), maxlen=10)\n",
        "print(\"\\nPadding and maxlen=10:\")\n",
        "print(padded_sentences_maxlen)\n",
        "\n",
        "# Example with padding, maxlen, and truncating\n",
        "padded_sentences_truncating = pad_sequences(tokenizer.texts_to_sequences(sentences), maxlen=5, truncating='post')\n",
        "print(\"\\nPadding, maxlen=5, and truncating='post':\")\n",
        "padded_sentences_truncating\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9H7723I63DA",
        "outputId": "80d3ba59-8b45-4cf9-f1b7-8219e1e8bbc1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padding only:\n",
            "[[3 4 5 1 1 1 1 2 1 1]\n",
            " [0 0 0 0 0 0 1 2 1 1]]\n",
            "\n",
            "Padding and maxlen=10:\n",
            "[[3 4 5 1 1 1 1 2 1 1]\n",
            " [0 0 0 0 0 0 1 2 1 1]]\n",
            "\n",
            "Padding, maxlen=5, and truncating='post':\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 4, 5, 1, 1],\n",
              "       [0, 1, 2, 1, 1]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemma stemm stopwords"
      ],
      "metadata": {
        "id": "5kEiGQEm7Wkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming"
      ],
      "metadata": {
        "id": "U-fGTsKtBupq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer('spanish')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6yPbn7cO7V4v",
        "outputId": "14642619-4198-4c61-9d43-3a526be86a14"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'soy muy proclive a enroscarme en soluciones que no funcion'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in sentences:\n",
        "  for w in sentences[0].split():\n",
        "      print(f\"{w} --> {stemmer.stem(w)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCBVsK7nBQw-",
        "outputId": "fc2b5b6b-b095-4281-eb42-74feae99d9ec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Soy --> soy\n",
            "muy --> muy\n",
            "proclive --> procliv\n",
            "a --> a\n",
            "enroscarme --> enrosc\n",
            "en --> en\n",
            "soluciones --> solucion\n",
            "que --> que\n",
            "no --> no\n",
            "funcionan --> funcion\n",
            "Soy --> soy\n",
            "muy --> muy\n",
            "proclive --> procliv\n",
            "a --> a\n",
            "enroscarme --> enrosc\n",
            "en --> en\n",
            "soluciones --> solucion\n",
            "que --> que\n",
            "no --> no\n",
            "funcionan --> funcion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatazing"
      ],
      "metadata": {
        "id": "FK19wz48BzIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS_7mhkpBstO",
        "outputId": "5d4442d2-f3f0-4683-e0c9-cce5b5993559"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sentences[0])\n",
        "type(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpfcUCUsCiPa",
        "outputId": "e17a545e-4585-43d6-d16f-3a1fa5b3f2f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35be382c",
        "outputId": "7d947a74-5c11-44a6-a540-7cbc7ecee5c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for s in sentences:\n",
        "  for w in doc:\n",
        "    lemma = w.lemma_\n",
        "    print(f\"{w} --> {lemma}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Soy --> ser\n",
            "muy --> mucho\n",
            "proclive --> proclive\n",
            "a --> a\n",
            "enroscarme --> enroscar yo\n",
            "en --> en\n",
            "soluciones --> solución\n",
            "que --> que\n",
            "no --> no\n",
            "funcionan --> funcionar\n",
            "Soy --> ser\n",
            "muy --> mucho\n",
            "proclive --> proclive\n",
            "a --> a\n",
            "enroscarme --> enroscar yo\n",
            "en --> en\n",
            "soluciones --> solución\n",
            "que --> que\n",
            "no --> no\n",
            "funcionan --> funcionar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords"
      ],
      "metadata": {
        "id": "_5df0caCDhc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwordlist = nlp.Defaults.stop_words\n",
        "print(stopwordlist)"
      ],
      "metadata": {
        "id": "TdNSpwb1DcRp",
        "outputId": "f0001d72-1011-4ce1-816c-2a3490efd364",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hay', 'cinco', 'mal', 'tampoco', 'cierto', 'aún', 'algo', 'vuestro', 'ha', 'quizas', 'mas', 'está', 'cada', 'sabemos', 'sabe', 'muy', 'tengo', 'cuantos', 'dado', 'entre', 'ti', 'si', 'para', 'fui', 'solas', 'alrededor', 'existen', 'estaba', 'detrás', 'podemos', 'va', 'haceis', 'algunas', 'decir', 'menos', 'sin', 'podeis', 'estais', 'mediante', 'así', 'durante', 'quienes', 'mi', 'lo', 'nosotros', 'en', 'habia', 'hoy', 'ninguno', 'otros', 'detras', 'cuantas', 'algún', 'tarde', 'éstos', 'primera', 'señaló', 'dos', 'bastante', 'partir', 'todos', 'aquello', 'cuales', 'cuál', 'ser', 'llegó', 'este', 'con', 'mio', 'todavía', 'unas', 'ademas', 'fueron', 'aun', 'he', 'solo', 'contra', 'nuestros', 'pueda', 'usa', 'proximo', 'tus', 'esos', 'tendrá', 'aquella', 'tienen', 'podriais', 'próximo', 'veces', 'eso', 'indicó', 'poner', 'ésta', 'mientras', 'demasiado', 'realizó', 'aquélla', 'hubo', 'primero', 'través', 'junto', 'aproximadamente', 'ellos', 'ir', 'dieron', 'temprano', 'nos', 'hasta', 'hace', 'de', 'parece', 'ambos', 'informó', 'consigues', 'haciendo', 'encuentra', 'mejor', 'tenía', 'tuyas', 'mismos', 'nuevo', 'mis', 'cierta', 'otras', 'cuanto', 'otro', 'sois', 'esto', 'diferente', 'dice', 'tenemos', 'mío', 'antes', 'días', 'vais', 'dio', 'debe', 'siendo', 'es', 'esas', 'tuyo', 'alguno', 'primer', 'bajo', 'aunque', 'mia', 'teneis', 'trata', 'sino', 'podrá', 'aquellas', 'cuánta', 'podria', 'usais', 'cuántas', 'paìs', 'hizo', 'habla', 'salvo', 'realizar', 'aquellos', 'final', 'estado', 'enfrente', 'nadie', 'tenido', 'explicó', 'había', 'después', 'al', 'él', 'ningunas', 'comentó', 'estar', 'hemos', 'su', 'afirmó', 'mías', 'hacen', 'que', 'ése', 'quiere', 'el', 'será', 'haya', 'dia', 'mayor', 'tanto', 'pronto', 'e', 'hacia', 'dan', 'ello', 'éstas', 'modo', 'suyas', 'doce', 'total', 'han', 'conseguir', 'cualquier', 'consiguen', 'sean', 'usamos', 'verdadera', 'buenos', 'estaban', 'son', 'once', 'nuestro', 'suyos', 'dijeron', 'ni', 'nuevas', 'poco', 'asi', 'buena', 'ahi', 'lado', 'éste', 'medio', 'informo', 'propio', 'añadió', 'serán', 'tal', 'quedó', 'del', 'sola', 'tuya', 'además', 'mencionó', 'saben', 'quiénes', 'conseguimos', 'ellas', 'le', 'tendrán', 'nueva', 'demás', 'algunos', 'somos', 'otra', 'dijo', 'eres', 'unos', 'hablan', 'propias', 'embargo', 'mios', 'yo', 'breve', 'eras', 'expresó', 'sabes', 'buen', 'poder', 'debajo', 'haber', 'solos', 'ahora', 'hago', 'contigo', 'mismo', 'tiene', 'más', 'uno', 'atras', 'incluso', 'hacemos', 'manifestó', 'ésos', 'último', 'diez', 'largo', 'pero', 'posible', 'segun', 'conocer', 'un', 'ya', 'cuánto', 'toda', 'últimos', 'podriamos', 'últimas', 'conmigo', 'fuimos', 'claro', 'creo', 'siempre', 'varias', 'ultimo', 'nuestra', 'tambien', 'mía', 'quizás', 'pocos', 'puede', 'cuanta', 'porque', 'vosotros', 'quiza', 'eran', 'siete', 'estuvo', 'despacio', 'mucha', 'dar', 'nosotras', 'casi', 'todo', 'nunca', 'menudo', 'usan', 'vosotras', 'míos', 'también', 'según', 'todas', 'nuevos', 'fuera', 'se', 'hicieron', 'quizá', 'estoy', 'y', 'tuvo', 'podrán', 'sí', 'vaya', 'solamente', 'fue', 'a', 'dias', 'haces', 'seis', 'te', 'acuerdo', 'delante', 'luego', 'habían', 'próximos', 'dicen', 'poca', 'pasada', 'bueno', 'saber', 'eramos', 'qeu', 'vuestros', 'aquí', 'sobre', 'desde', 'manera', 'aquél', 'última', 'pesar', 'usted', 'agregó', 'ocho', 'donde', 'vamos', 'nueve', 'alguna', 'segundo', 'las', 'propios', 'nada', 'usas', 'pueden', 'consigo', 'vez', 'sabeis', 'pasado', 'bien', 'estos', 'sera', 'siguiente', 'estará', 'por', 'consigue', 'dicho', 'primeros', 'aquéllos', 'aquel', 'una', 'propia', 'todavia', 'van', 'despues', 'puedo', 'esa', 'como', 'mucho', 'excepto', 'aseguró', 'podrian', 'habrá', 'cuándo', 'estamos', 'igual', 'los', 'muchas', 'varios', 'ella', 'soy', 'encima', 'les', 'tuyos', 'verdadero', 'ciertas', 'tan', 'estan', 'mias', 'peor', 'tú', 'hacerlo', 'dónde', 'sería', 'adelante', 'podrias', 'existe', 'cuáles', 'dejó', 'ese', 'están', 'podría', 'mí', 'buenas', 'usar', 'sido', 'hacer', 'sólo', 'repente', 'quién', 'no', 'segunda', 'respecto', 'sea', 'deben', 'me', 'apenas', 'da', 'arriba', 'estados', 'aquéllas', 'parte', 'queremos', 'cómo', 'considera', 'ésas', 'ahí', 'ustedes', 'podrían', 'ciertos', 'vuestras', 'debido', 'suya', 'grandes', 'qué', 'ante', 'consideró', 'vuestra', 'cuatro', 'dentro', 'nuestras', 'realizado', 'uso', 'la', 'cuenta', 'esta', 'os', 'verdad', 'deprisa', 'sé', 'cuántos', 'fin', 'era', 'supuesto', 'llevar', 'tener', 'suyo', 'gran', 'pudo', 'sus', 'lleva', 'ésa', 'tres', 'grande', 'diferentes', 'cual', 'ningunos', 'alli', 'entonces', 'tercera', 'u', 'ningún', 'tercero', 'misma', 'mismas', 'enseguida', 'sigue', 'ver', 'tras', 'voy', 'muchos', 'o', 'pocas', 'quien', 'pues', 'día', 'anterior', 'estas', 'tenga', 'allí', 'tu', 'aqui', 'ninguna', 'cuando', 'hecho'}\n"
          ]
        }
      ]
    }
  ]
}